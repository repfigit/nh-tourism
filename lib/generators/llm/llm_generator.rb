class LlmGenerator < Rails::Generators::Base
  source_root File.expand_path('templates', __dir__)

  desc "Generate LLM service with streaming and blocking API support"

  class_option :skip_service, type: :boolean, default: false, desc: "Skip service file generation"
  class_option :skip_job, type: :boolean, default: false, desc: "Skip job file generation"

  def create_service_file
    return if options[:skip_service]
    template 'llm_service.rb.erb', 'app/services/llm_service.rb'
  end

  def create_job_file
    return if options[:skip_job]
    template 'llm_stream_job.rb.erb', 'app/jobs/llm_stream_job.rb'
  end

  def create_llm_message_validation_concern
    template 'llm_message_validation_concern.rb.erb', 'app/models/concerns/llm_message_validation_concern.rb'
  end

  def update_application_yml
    llm_config = <<~YAML

      # LLM Service Configuration generated by llm generator
      LLM_BASE_URL: '<%= ENV.fetch("CLACKY_LLM_BASE_URL", '') %>'
      LLM_API_KEY: '<%= ENV.fetch("CLACKY_LLM_API_KEY", '') %>'
      LLM_MODEL: '<%= ENV.fetch("CLACKY_LLM_MODEL", 'gemini-2.5-flash') %>'
      # LLM Service Configuration generated end
    YAML

    # Update application.yml.example
    add_llm_config_to_file('config/application.yml.example', llm_config)

    # Update application.yml if it exists
    add_llm_config_to_file('config/application.yml', llm_config)
  end


  def show_usage_instructions
    say "LLM Generator completed successfully!", :green

    say "\nüìù Configuration:"
    say "  Environment variables added to config/application.yml.example"
    say "  Configure these in your config/application.yml:"
    say "    LLM_BASE_URL     - API endpoint (e.g., https://api.openai.com/v1)"
    say "    LLM_API_KEY      - Your API key"
    say "    LLM_MODEL        - Model name (e.g., gpt-4o-mini, deepseek-chat)"

    say "\nüöÄ Usage (Streaming via ActionCable):"
    say "     LlmStreamJob.perform_later("
    say "       channel_name: \"chat_\#{user_id}\","
    say "       prompt: 'Explain quantum computing',"
    say "       system: 'You are a helpful assistant'"
    say "     )"

    say "\nüìö Next Steps:"
    say "  1. Configure your API keys in config/application.yml"
    say "  2. Use LlmStreamJob with ActionCable for real-time streaming"

    say "\nü§ñ AI Assistant Note:"
    say "  When storing LLM messages, include LlmMessageValidationConcern."
    say "  Don't validate role & content yourself - the concern handles it."
  end

  private

  def add_llm_config_to_file(file_path, llm_config)
    if File.exist?(file_path)
      content = File.read(file_path)

      unless content.include?('# LLM Service Configuration generated by llm generator')
        append_to_file file_path, llm_config
        say "Added LLM configuration to #{File.basename(file_path)}", :green
      else
        say "LLM configuration already exists in #{File.basename(file_path)}, skipping...", :yellow
      end
    else
      say "#{File.basename(file_path)} not found, skipping...", :yellow
    end
  end
end
