class LlmStreamJob < ApplicationJob
  queue_as :llm

  # Retry strategy configuration
  retry_on Net::ReadTimeout, wait: 5.seconds, attempts: 3
  retry_on LlmService::TimeoutError, wait: 5.seconds, attempts: 3
  retry_on LlmService::ApiError, wait: 10.seconds, attempts: 2

  discard_on ActiveJob::DeserializationError

  # Streaming LLM responses via ActionCable
  # Usage: LlmStreamJob.perform_later(channel_name: "chat_#{user_id}", prompt: "Hello")
  #
  # CRITICAL: ALL broadcasts MUST have 'type' field (auto-routes to client handler)
  # - type: 'chunk' → client calls handleChunk(data)
  # - type: 'done' → client calls handleDone(data)
  # - type: 'system-error' → handled by error_handler.ts
  def perform(channel_name:, prompt:, system: nil, **options)
    full_content = ""

    # Stream LLM response chunk by chunk
    LlmService.call(prompt: prompt, system: system, **options) do |chunk|
      full_content += chunk

      # Broadcast each chunk → client handleChunk(data)
      ActionCable.server.broadcast(channel_name, {
        type: 'chunk',  # REQUIRED: routes to handleChunk() method
        chunk: chunk
      })
    end

    # Broadcast completion → client handleDone(data)
    ActionCable.server.broadcast(channel_name, {
      type: 'done',  # REQUIRED: routes to handleDone() method
      content: full_content
    })
  rescue => e
    Rails.logger.error("LlmStreamJob failed: #{e.class} - #{e.message}")

    # Broadcast error → handled by base_channel_controller.ts
    ActionCable.server.broadcast(channel_name, {
      type: 'system-error',
      message: e.message
    })

    raise
  end
end
